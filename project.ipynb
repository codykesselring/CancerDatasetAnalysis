{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b2969",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/ORoof/OneDrive/Documents/GitHub/CancerDatasetAnalysis/.venv/bin/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import mysklearn.mypytable\n",
    "import importlib\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyNaiveBayesClassifier, MyRandomForestClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import matplotlib.pyplot\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd375d",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89038ac-6df3-4543-823d-640d91325ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_table = MyPyTable()\n",
    "cancer_table.load_from_file('input_data/cancer.csv')\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "all_features = []\n",
    "for row in cancer_table.data:\n",
    "    if row[1] in ['M', 'B']:\n",
    "        feature_values = [float(row[i]) for i in range(2, 32)]  # 30 features\n",
    "        all_features.append(feature_values)\n",
    "\n",
    "# quartiles for each feature\n",
    "import numpy as np\n",
    "quartiles = []\n",
    "for feature_idx in range(30):\n",
    "    feature_column = [row[feature_idx] for row in all_features]\n",
    "    q1 = np.percentile(feature_column, 25)\n",
    "    q2 = np.percentile(feature_column, 50)\n",
    "    q3 = np.percentile(feature_column, 75)\n",
    "    quartiles.append((q1, q2, q3))\n",
    "\n",
    "# discretize features based on quartiles\n",
    "for row in cancer_table.data:\n",
    "    if row[1] in ['M', 'B']:\n",
    "        features = []\n",
    "        for feature_idx in range(30):\n",
    "            value = float(row[feature_idx + 2])  \n",
    "            q1, q2, q3 = quartiles[feature_idx]\n",
    "\n",
    "            # 4 bins based on quartiles\n",
    "            if value <= q1:\n",
    "                features.append(f\"f{feature_idx}_low\")\n",
    "            elif value <= q2:\n",
    "                features.append(f\"f{feature_idx}_med_low\")\n",
    "            elif value <= q3:\n",
    "                features.append(f\"f{feature_idx}_med_high\")\n",
    "            else:\n",
    "                features.append(f\"f{feature_idx}_high\")\n",
    "\n",
    "        X.append(features)\n",
    "        y.append(row[1])  # M or B\n",
    "\n",
    "print(f\"Loaded {len(X)} instances from cancer dataset\")\n",
    "print(f\"Sample X: {X[0]}, y: {y[0]}\")\n",
    "print(f\"Sample X: {X[1]}, y: {y[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb788f",
   "metadata": {},
   "source": [
    "### Visualizing Class Distriubtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2284479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Counter(y)\n",
    "labels = ['Benign (B)', 'Malignant (M)']\n",
    "sizes = [class_counts['B'], class_counts['M']]\n",
    "\n",
    "# Pie chart distribution\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "plt.title(\"Distribution of Tumor Types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c913c27-f882-4d47-9b9c-10cf97ec349c",
   "metadata": {},
   "source": [
    "### K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a8134-fa53-49a4-be0f-e20002552faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = myevaluation.kfold_split(X, n_splits=10, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f401577",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42aa9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_all_y_true = []\n",
    "nb_all_y_pred = []\n",
    "\n",
    "print(\"Running Naive Bayes with 10-fold cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    nb_clf = MyNaiveBayesClassifier()\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "    nb_all_y_true.extend(y_test)\n",
    "    nb_all_y_pred.extend(y_pred)\n",
    "\n",
    "    fold_accuracy = myevaluation.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Fold {fold_idx + 1}: Accuracy = {fold_accuracy:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "nb_accuracy = myevaluation.accuracy_score(nb_all_y_true, nb_all_y_pred)\n",
    "nb_error_rate = 1 - nb_accuracy\n",
    "\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Overall Error Rate: {nb_error_rate:.4f}\")\n",
    "\n",
    "labels = [\"B\", \"M\"]\n",
    "\n",
    "nb_precision = myevaluation.binary_precision_score(nb_all_y_true, nb_all_y_pred,\n",
    "                                                     labels=labels, pos_label=\"M\")\n",
    "nb_recall = myevaluation.binary_recall_score(nb_all_y_true, nb_all_y_pred,\n",
    "                                               labels=labels, pos_label=\"M\")\n",
    "nb_f1 = myevaluation.binary_f1_score(nb_all_y_true, nb_all_y_pred,\n",
    "                                      labels=labels, pos_label=\"M\")\n",
    "\n",
    "print(f\"\\nPrecision (Malignant): {nb_precision:.4f}\")\n",
    "print(f\"Recall (Malignant): {nb_recall:.4f}\")\n",
    "print(f\"F1 Score (Malignant): {nb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf92c88",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f715bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_all_y_true = []\n",
    "rf_all_y_pred = []\n",
    "\n",
    "print(\"Running Random Forest with 10-fold cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    rf_clf = MyRandomForestClassifier(n_trees=20, m_trees=7, f_attributes=10, random_state=0)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "    rf_all_y_true.extend(y_test)\n",
    "    rf_all_y_pred.extend(y_pred)\n",
    "\n",
    "    fold_accuracy = myevaluation.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Fold {fold_idx + 1}: Accuracy = {fold_accuracy:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_accuracy = myevaluation.accuracy_score(rf_all_y_true, rf_all_y_pred)\n",
    "rf_error_rate = 1 - rf_accuracy\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Overall Error Rate: {rf_error_rate:.4f}\")\n",
    "\n",
    "rf_precision = myevaluation.binary_precision_score(rf_all_y_true, rf_all_y_pred,\n",
    "                                                     labels=labels, pos_label=\"M\")\n",
    "rf_recall = myevaluation.binary_recall_score(rf_all_y_true, rf_all_y_pred,\n",
    "                                               labels=labels, pos_label=\"M\")\n",
    "rf_f1 = myevaluation.binary_f1_score(rf_all_y_true, rf_all_y_pred,\n",
    "                                      labels=labels, pos_label=\"M\")\n",
    "\n",
    "print(f\"\\nPrecision (Malignant): {rf_precision:.4f}\")\n",
    "print(f\"Recall (Malignant): {rf_recall:.4f}\")\n",
    "print(f\"F1 Score (Malignant): {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f509b1a",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_all_y_true = []\n",
    "dt_all_y_pred = []\n",
    "\n",
    "print(\"Running Decision Tree with 10-fold cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    dt_clf = MyDecisionTreeClassifier()\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "    dt_all_y_true.extend(y_test)\n",
    "    dt_all_y_pred.extend(y_pred)\n",
    "\n",
    "    fold_accuracy = myevaluation.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Fold {fold_idx + 1}: Accuracy = {fold_accuracy:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate overall metrics for Decision Tree\n",
    "dt_accuracy = myevaluation.accuracy_score(dt_all_y_true, dt_all_y_pred)\n",
    "dt_error_rate = 1 - dt_accuracy\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Overall Error Rate: {dt_error_rate:.4f}\")\n",
    "\n",
    "dt_precision = myevaluation.binary_precision_score(dt_all_y_true, dt_all_y_pred,\n",
    "                                                     labels=labels, pos_label=\"M\")\n",
    "dt_recall = myevaluation.binary_recall_score(dt_all_y_true, dt_all_y_pred,\n",
    "                                               labels=labels, pos_label=\"M\")\n",
    "dt_f1 = myevaluation.binary_f1_score(dt_all_y_true, dt_all_y_pred,\n",
    "                                      labels=labels, pos_label=\"M\")\n",
    "\n",
    "print(f\"\\nPrecision (Malignant): {dt_precision:.4f}\")\n",
    "print(f\"Recall (Malignant): {dt_recall:.4f}\")\n",
    "print(f\"F1 Score (Malignant): {dt_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81edafb1",
   "metadata": {},
   "source": [
    "### Visualizations of Classifier Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier comparison bar chart\n",
    "metrics = ['Accuracy', 'Precision (M)', 'Recall (M)', 'F1 Score (M)']\n",
    "nb_vals = [nb_accuracy, nb_precision, nb_recall, nb_f1]\n",
    "rf_vals = [rf_accuracy, rf_precision, rf_recall, rf_f1]\n",
    "dt_vals = [dt_accuracy, dt_precision, dt_recall, dt_f1]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(x - width, nb_vals, width, label='Naive Bayes')\n",
    "plt.bar(x, rf_vals, width, label='Random Forest')\n",
    "plt.bar(x + width, dt_vals, width, label='Decision Tree')\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Classifier Performance Comparison\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "top_5_indices = [23, 27, 7, 20, 6]\n",
    "top_5_names = ['Area Worst', 'Concave Pts Worst', 'Concave Pts Mean', 'Radius Worst', 'Concavity Mean']\n",
    "\n",
    "# Create a figure with 5 subplots arranged in a row\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "for i, (feature_idx, feature_name) in enumerate(zip(top_5_indices, top_5_names)):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    \n",
    "    # Extract data for Benign and Malignant for the current feature\n",
    "    b_data = [row[feature_idx] for row, label in zip(all_features, y) if label == 'B']\n",
    "    m_data = [row[feature_idx] for row, label in zip(all_features, y) if label == 'M']\n",
    "    \n",
    "    # Create the boxplot\n",
    "    plt.boxplot([b_data, m_data], labels=['Benign', 'Malignant'], patch_artist=True)\n",
    "    plt.title(feature_name)\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1042f2c",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_confusion_matrix = myevaluation.confusion_matrix(dt_all_y_true, dt_all_y_pred, labels)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"{'':12} {'Predicted B':>15} {'Predicted M':>15}\")\n",
    "print(f\"{'Actual B':12} {dt_confusion_matrix[0][0]:>15} {dt_confusion_matrix[0][1]:>15}\")\n",
    "print(f\"{'Actual M':12} {dt_confusion_matrix[1][0]:>15} {dt_confusion_matrix[1][1]:>15}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comparison Summary\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFIER COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'Naive Bayes':>15} {'Random Forest':>15} {'Decision Tree':>15}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"{'Accuracy':<25} {nb_accuracy:>15.4f} {rf_accuracy:>15.4f} {dt_accuracy:>15.4f}\")\n",
    "print(f\"{'Error Rate':<25} {nb_error_rate:>15.4f} {rf_error_rate:>15.4f} {dt_error_rate:>15.4f}\")\n",
    "print(f\"{'Precision (Malignant)':<25} {nb_precision:>15.4f} {rf_precision:>15.4f} {dt_precision:>15.4f}\")\n",
    "print(f\"{'Recall (Malignant)':<25} {nb_recall:>15.4f} {rf_recall:>15.4f} {dt_recall:>15.4f}\")\n",
    "print(f\"{'F1 Score (Malignant)':<25} {nb_f1:>15.4f} {rf_f1:>15.4f} {dt_f1:>15.4f}\")\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03856b9e",
   "metadata": {},
   "source": [
    "### More Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd499fae-d209-4f6c-8cba-61bddc76806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AREA_WORST_IDX = 23\n",
    "CONCAVE_PTS_IDX = 27\n",
    "\n",
    "m_area_worst = [row[AREA_WORST_IDX] for row, label in zip(all_features, y) if label == 'M']\n",
    "m_concave_pts = [row[CONCAVE_PTS_IDX] for row, label in zip(all_features, y) if label == 'M']\n",
    "\n",
    "b_area_worst = [row[AREA_WORST_IDX] for row, label in zip(all_features, y) if label == 'B']\n",
    "b_concave_pts = [row[CONCAVE_PTS_IDX] for row, label in zip(all_features, y) if label == 'B']\n",
    "\n",
    "# Graph 1: Scatter Plot of Top 2 Features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(b_area_worst, b_concave_pts, color='green', label='Benign', alpha=0.5)\n",
    "plt.scatter(m_area_worst, m_concave_pts, color='red', label='Malignant', alpha=0.5)\n",
    "\n",
    "plt.title(\"Scatter Plot: Area Worst vs. Concave Points Worst\")\n",
    "plt.xlabel(\"Area Worst\")\n",
    "plt.ylabel(\"Concave Points Worst\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Histogram of Concave Points Worst\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(b_concave_pts, bins=20, color='green', alpha=0.5, label='Benign', density=True)\n",
    "plt.hist(m_concave_pts, bins=20, color='red', alpha=0.5, label='Malignant', density=True)\n",
    "\n",
    "plt.title(\"Distribution of Concave Points Worst by Diagnosis\")\n",
    "plt.xlabel(\"Concave Points Worst\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
