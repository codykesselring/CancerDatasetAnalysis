{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b2969",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/ORoof/OneDrive/Documents/GitHub/CancerDatasetAnalysis/.venv/bin/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import mysklearn.mypytable\n",
    "import importlib\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyNaiveBayesClassifier, MyRandomForestClassifier, MyDecisionTreeClassifier, MyDummyClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import matplotlib.pyplot\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd375d",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89038ac-6df3-4543-823d-640d91325ed0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 instances from cancer dataset\n",
      "Sample X: ['f0_high', 'f1_low', 'f2_high', 'f3_high', 'f4_high', 'f5_high', 'f6_high', 'f7_high', 'f8_high', 'f9_high', 'f10_high', 'f11_med_low', 'f12_high', 'f13_high', 'f14_med_high', 'f15_high', 'f16_high', 'f17_high', 'f18_high', 'f19_high', 'f20_high', 'f21_low', 'f22_high', 'f23_high', 'f24_high', 'f25_high', 'f26_high', 'f27_high', 'f28_high', 'f29_high'], y: M\n",
      "Sample X: ['f0_high', 'f1_med_low', 'f2_high', 'f3_high', 'f4_low', 'f5_med_low', 'f6_med_high', 'f7_med_high', 'f8_med_high', 'f9_low', 'f10_med_high', 'f11_low', 'f12_med_high', 'f13_high', 'f14_low', 'f15_low', 'f16_med_low', 'f17_med_high', 'f18_low', 'f19_med_high', 'f20_high', 'f21_med_low', 'f22_high', 'f23_high', 'f24_med_low', 'f25_med_low', 'f26_med_low', 'f27_high', 'f28_med_low', 'f29_med_high'], y: M\n"
     ]
    }
   ],
   "source": [
    "cancer_table = MyPyTable()\n",
    "cancer_table.load_from_file('input_data/cancer_augmented.csv')\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "all_features = []\n",
    "for row in cancer_table.data:\n",
    "    if row[1] in ['M', 'B']:\n",
    "        feature_values = [float(row[i]) for i in range(2, 32)]  # 30 features\n",
    "        all_features.append(feature_values)\n",
    "\n",
    "# quartiles for each feature\n",
    "import numpy as np\n",
    "quartiles = []\n",
    "for feature_idx in range(30):\n",
    "    feature_column = [row[feature_idx] for row in all_features]\n",
    "    q1 = np.percentile(feature_column, 25)\n",
    "    q2 = np.percentile(feature_column, 50)\n",
    "    q3 = np.percentile(feature_column, 75)\n",
    "    quartiles.append((q1, q2, q3))\n",
    "\n",
    "# discretize features based on quartiles\n",
    "for row in cancer_table.data:\n",
    "    if row[1] in ['M', 'B']:\n",
    "        features = []\n",
    "        for feature_idx in range(30):\n",
    "            value = float(row[feature_idx + 2])  \n",
    "            q1, q2, q3 = quartiles[feature_idx]\n",
    "\n",
    "            # 4 bins based on quartiles\n",
    "            if value <= q1:\n",
    "                features.append(f\"f{feature_idx}_low\")\n",
    "            elif value <= q2:\n",
    "                features.append(f\"f{feature_idx}_med_low\")\n",
    "            elif value <= q3:\n",
    "                features.append(f\"f{feature_idx}_med_high\")\n",
    "            else:\n",
    "                features.append(f\"f{feature_idx}_high\")\n",
    "\n",
    "        X.append(features)\n",
    "        y.append(row[1])  # M or B\n",
    "\n",
    "print(f\"Loaded {len(X)} instances from cancer dataset\")\n",
    "print(f\"Sample X: {X[0]}, y: {y[0]}\")\n",
    "print(f\"Sample X: {X[1]}, y: {y[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb788f",
   "metadata": {},
   "source": [
    "### Visualizing Class Distriubtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2284479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Counter(y)\n",
    "labels = ['Benign (B)', 'Malignant (M)']\n",
    "sizes = [class_counts['B'], class_counts['M']]\n",
    "\n",
    "# Pie chart distribution\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "plt.title(\"Distribution of Tumor Types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c913c27-f882-4d47-9b9c-10cf97ec349c",
   "metadata": {},
   "source": [
    "### K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a8134-fa53-49a4-be0f-e20002552faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = myevaluation.kfold_split(X, n_splits=10, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f401577",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42aa9a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Naive Bayes with 10-fold cross-validation...\n",
      "============================================================\n",
      "Fold 1: Accuracy = 0.9600\n",
      "Fold 2: Accuracy = 0.9400\n",
      "Fold 3: Accuracy = 0.9100\n",
      "Fold 4: Accuracy = 0.9500\n",
      "Fold 5: Accuracy = 0.9400\n",
      "Fold 6: Accuracy = 0.9200\n",
      "Fold 7: Accuracy = 0.9500\n",
      "Fold 8: Accuracy = 0.9000\n",
      "Fold 9: Accuracy = 0.9400\n",
      "Fold 10: Accuracy = 0.9500\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.9360\n",
      "Overall Error Rate: 0.0640\n",
      "\n",
      "Precision (Malignant): 0.9504\n",
      "Recall (Malignant): 0.9200\n",
      "F1 Score (Malignant): 0.9350\n"
     ]
    }
   ],
   "source": [
    "nb_all_y_true = []\n",
    "nb_all_y_pred = []\n",
    "\n",
    "print(\"Running Naive Bayes with 10-fold cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    nb_clf = MyNaiveBayesClassifier()\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "    nb_all_y_true.extend(y_test)\n",
    "    nb_all_y_pred.extend(y_pred)\n",
    "\n",
    "    fold_accuracy = myevaluation.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Fold {fold_idx + 1}: Accuracy = {fold_accuracy:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "nb_accuracy = myevaluation.accuracy_score(nb_all_y_true, nb_all_y_pred)\n",
    "nb_error_rate = 1 - nb_accuracy\n",
    "\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Overall Error Rate: {nb_error_rate:.4f}\")\n",
    "\n",
    "labels = [\"B\", \"M\"]\n",
    "\n",
    "nb_precision = myevaluation.binary_precision_score(nb_all_y_true, nb_all_y_pred,\n",
    "                                                     labels=labels, pos_label=\"M\")\n",
    "nb_recall = myevaluation.binary_recall_score(nb_all_y_true, nb_all_y_pred,\n",
    "                                               labels=labels, pos_label=\"M\")\n",
    "nb_f1 = myevaluation.binary_f1_score(nb_all_y_true, nb_all_y_pred,\n",
    "                                      labels=labels, pos_label=\"M\")\n",
    "\n",
    "print(f\"\\nPrecision (Malignant): {nb_precision:.4f}\")\n",
    "print(f\"Recall (Malignant): {nb_recall:.4f}\")\n",
    "print(f\"F1 Score (Malignant): {nb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf92c88",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f715bff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random Forest with 10-fold cross-validation...\n",
      "============================================================\n",
      "Fold 1: Accuracy = 0.9800\n",
      "Fold 2: Accuracy = 0.9800\n",
      "Fold 3: Accuracy = 0.9200\n",
      "Fold 4: Accuracy = 0.9400\n",
      "Fold 5: Accuracy = 0.9800\n",
      "Fold 6: Accuracy = 0.9700\n",
      "Fold 7: Accuracy = 0.9800\n",
      "Fold 8: Accuracy = 0.9600\n",
      "Fold 9: Accuracy = 0.9900\n",
      "Fold 10: Accuracy = 0.9600\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.9660\n",
      "Overall Error Rate: 0.0340\n",
      "\n",
      "Precision (Malignant): 0.9755\n",
      "Recall (Malignant): 0.9560\n",
      "F1 Score (Malignant): 0.9657\n"
     ]
    }
   ],
   "source": [
    "rf_all_y_true = []\n",
    "rf_all_y_pred = []\n",
    "\n",
    "print(\"Running Random Forest with 10-fold cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    rf_clf = MyRandomForestClassifier(n_trees=20, m_trees=7, f_attributes=10, random_state=0)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "    rf_all_y_true.extend(y_test)\n",
    "    rf_all_y_pred.extend(y_pred)\n",
    "\n",
    "    fold_accuracy = myevaluation.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Fold {fold_idx + 1}: Accuracy = {fold_accuracy:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_accuracy = myevaluation.accuracy_score(rf_all_y_true, rf_all_y_pred)\n",
    "rf_error_rate = 1 - rf_accuracy\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Overall Error Rate: {rf_error_rate:.4f}\")\n",
    "\n",
    "rf_precision = myevaluation.binary_precision_score(rf_all_y_true, rf_all_y_pred,\n",
    "                                                     labels=labels, pos_label=\"M\")\n",
    "rf_recall = myevaluation.binary_recall_score(rf_all_y_true, rf_all_y_pred,\n",
    "                                               labels=labels, pos_label=\"M\")\n",
    "rf_f1 = myevaluation.binary_f1_score(rf_all_y_true, rf_all_y_pred,\n",
    "                                      labels=labels, pos_label=\"M\")\n",
    "\n",
    "print(f\"\\nPrecision (Malignant): {rf_precision:.4f}\")\n",
    "print(f\"Recall (Malignant): {rf_recall:.4f}\")\n",
    "print(f\"F1 Score (Malignant): {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f509b1a",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e8600",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Decision Tree with 10-fold cross-validation...\n",
      "============================================================\n",
      "Fold 1: Accuracy = 0.9800\n",
      "Fold 2: Accuracy = 0.9700\n",
      "Fold 3: Accuracy = 0.9400\n",
      "Fold 4: Accuracy = 0.9200\n",
      "Fold 5: Accuracy = 0.9400\n",
      "Fold 6: Accuracy = 0.9900\n",
      "Fold 7: Accuracy = 0.9600\n",
      "Fold 8: Accuracy = 0.9300\n",
      "Fold 9: Accuracy = 0.9600\n",
      "Fold 10: Accuracy = 0.9600\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.9550\n",
      "Overall Error Rate: 0.0450\n",
      "\n",
      "Precision (Malignant): 0.9559\n",
      "Recall (Malignant): 0.9540\n",
      "F1 Score (Malignant): 0.9550\n"
     ]
    }
   ],
   "source": [
    "dt_all_y_true = []\n",
    "dt_all_y_pred = []\n",
    "\n",
    "print(\"Running Decision Tree with 10-fold cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    dt_clf = MyDecisionTreeClassifier()\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "    dt_all_y_true.extend(y_test)\n",
    "    dt_all_y_pred.extend(y_pred)\n",
    "\n",
    "    fold_accuracy = myevaluation.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Fold {fold_idx + 1}: Accuracy = {fold_accuracy:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate overall metrics for Decision Tree\n",
    "dt_accuracy = myevaluation.accuracy_score(dt_all_y_true, dt_all_y_pred)\n",
    "dt_error_rate = 1 - dt_accuracy\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Overall Error Rate: {dt_error_rate:.4f}\")\n",
    "\n",
    "dt_precision = myevaluation.binary_precision_score(dt_all_y_true, dt_all_y_pred,\n",
    "                                                     labels=labels, pos_label=\"M\")\n",
    "dt_recall = myevaluation.binary_recall_score(dt_all_y_true, dt_all_y_pred,\n",
    "                                               labels=labels, pos_label=\"M\")\n",
    "dt_f1 = myevaluation.binary_f1_score(dt_all_y_true, dt_all_y_pred,\n",
    "                                      labels=labels, pos_label=\"M\")\n",
    "\n",
    "print(f\"\\nPrecision (Malignant): {dt_precision:.4f}\")\n",
    "print(f\"Recall (Malignant): {dt_recall:.4f}\")\n",
    "print(f\"F1 Score (Malignant): {dt_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vmrwznd0r8c",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "jpxcoxzzsp8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Dummy Classifier with 10-fold cross-validation...\n",
      "============================================================\n",
      "Fold 1: Accuracy = 0.4900\n",
      "Fold 2: Accuracy = 0.4600\n",
      "Fold 3: Accuracy = 0.4800\n",
      "Fold 4: Accuracy = 0.4400\n",
      "Fold 5: Accuracy = 0.3900\n",
      "Fold 6: Accuracy = 0.4400\n",
      "Fold 7: Accuracy = 0.4900\n",
      "Fold 8: Accuracy = 0.5000\n",
      "Fold 9: Accuracy = 0.4800\n",
      "Fold 10: Accuracy = 0.4700\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.4640\n",
      "Overall Error Rate: 0.5360\n",
      "\n",
      "Precision (Malignant): 0.4550\n",
      "Recall (Malignant): 0.3640\n",
      "F1 Score (Malignant): 0.4044\n"
     ]
    }
   ],
   "source": [
    "dummy_all_y_true = []\n",
    "dummy_all_y_pred = []\n",
    "\n",
    "print(\"Running Dummy Classifier with 10-fold cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    dummy_clf = MyDummyClassifier()\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "    dummy_all_y_true.extend(y_test)\n",
    "    dummy_all_y_pred.extend(y_pred)\n",
    "\n",
    "    fold_accuracy = myevaluation.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Fold {fold_idx + 1}: Accuracy = {fold_accuracy:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate overall metrics for Dummy Classifier\n",
    "dummy_accuracy = myevaluation.accuracy_score(dummy_all_y_true, dummy_all_y_pred)\n",
    "dummy_error_rate = 1 - dummy_accuracy\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {dummy_accuracy:.4f}\")\n",
    "print(f\"Overall Error Rate: {dummy_error_rate:.4f}\")\n",
    "\n",
    "dummy_precision = myevaluation.binary_precision_score(dummy_all_y_true, dummy_all_y_pred,\n",
    "                                                     labels=labels, pos_label=\"M\")\n",
    "dummy_recall = myevaluation.binary_recall_score(dummy_all_y_true, dummy_all_y_pred,\n",
    "                                               labels=labels, pos_label=\"M\")\n",
    "dummy_f1 = myevaluation.binary_f1_score(dummy_all_y_true, dummy_all_y_pred,\n",
    "                                      labels=labels, pos_label=\"M\")\n",
    "\n",
    "print(f\"\\nPrecision (Malignant): {dummy_precision:.4f}\")\n",
    "print(f\"Recall (Malignant): {dummy_recall:.4f}\")\n",
    "print(f\"F1 Score (Malignant): {dummy_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1042f2c",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24de21d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "CLASSIFIER COMPARISON SUMMARY\n",
      "============================================================\n",
      "Metric                     Naive Bayes  Random Forest  Decision Tree        Dummy\n",
      "------------------------------------------------------------------------------------------\n",
      "Accuracy                        0.9360         0.9660         0.9550       0.4640\n",
      "Error Rate                      0.0640         0.0340         0.0450       0.5360\n",
      "Precision (Malignant)           0.9504         0.9755         0.9559       0.4550\n",
      "Recall (Malignant)              0.9200         0.9560         0.9540       0.3640\n",
      "F1 Score (Malignant)            0.9350         0.9657         0.9550       0.4044\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "dt_confusion_matrix = myevaluation.confusion_matrix(dt_all_y_true, dt_all_y_pred, labels)\n",
    "dummy_confusion_matrix = myevaluation.confusion_matrix(dummy_all_y_true, dummy_all_y_pred, labels)\n",
    "\n",
    "# Comparison Summary\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFIER COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'Naive Bayes':>12} {'Random Forest':>14} {'Decision Tree':>14} {'Dummy':>12}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"{'Accuracy':<25} {nb_accuracy:>12.4f} {rf_accuracy:>14.4f} {dt_accuracy:>14.4f} {dummy_accuracy:>12.4f}\")\n",
    "print(f\"{'Error Rate':<25} {nb_error_rate:>12.4f} {rf_error_rate:>14.4f} {dt_error_rate:>14.4f} {dummy_error_rate:>12.4f}\")\n",
    "print(f\"{'Precision (Malignant)':<25} {nb_precision:>12.4f} {rf_precision:>14.4f} {dt_precision:>14.4f} {dummy_precision:>12.4f}\")\n",
    "print(f\"{'Recall (Malignant)':<25} {nb_recall:>12.4f} {rf_recall:>14.4f} {dt_recall:>14.4f} {dummy_recall:>12.4f}\")\n",
    "print(f\"{'F1 Score (Malignant)':<25} {nb_f1:>12.4f} {rf_f1:>14.4f} {dt_f1:>14.4f} {dummy_f1:>12.4f}\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03856b9e",
   "metadata": {},
   "source": [
    "### More Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd499fae-d209-4f6c-8cba-61bddc76806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AREA_WORST_IDX = 23\n",
    "CONCAVE_PTS_IDX = 27\n",
    "\n",
    "m_area_worst = [row[AREA_WORST_IDX] for row, label in zip(all_features, y) if label == 'M']\n",
    "m_concave_pts = [row[CONCAVE_PTS_IDX] for row, label in zip(all_features, y) if label == 'M']\n",
    "\n",
    "b_area_worst = [row[AREA_WORST_IDX] for row, label in zip(all_features, y) if label == 'B']\n",
    "b_concave_pts = [row[CONCAVE_PTS_IDX] for row, label in zip(all_features, y) if label == 'B']\n",
    "\n",
    "# Graph 1: Scatter Plot of Top 2 Features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(b_area_worst, b_concave_pts, color='green', label='Benign', alpha=0.5)\n",
    "plt.scatter(m_area_worst, m_concave_pts, color='red', label='Malignant', alpha=0.5)\n",
    "\n",
    "plt.title(\"Scatter Plot: Area Worst vs. Concave Points Worst\")\n",
    "plt.xlabel(\"Area Worst\")\n",
    "plt.ylabel(\"Concave Points Worst\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Histogram of Concave Points Worst\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(b_concave_pts, bins=20, color='green', alpha=0.5, label='Benign', density=True)\n",
    "plt.hist(m_concave_pts, bins=20, color='red', alpha=0.5, label='Malignant', density=True)\n",
    "\n",
    "plt.title(\"Distribution of Concave Points Worst by Diagnosis\")\n",
    "plt.xlabel(\"Concave Points Worst\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
